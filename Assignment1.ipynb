{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "\n",
    "Link to github: https://github.com/NikolajT84/CSS_assignment1\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "We have written the code working together on Nikolaj's machine, which is why all commits are from the same user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "175b6d70c0d147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports for all exercises\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T13:48:56.952404400Z",
     "start_time": "2024-02-21T13:48:56.737290300Z"
    }
   },
   "id": "75a3d5f27b9b93b1",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1\n",
    "\n",
    "We follow the instructions, and use bs4 to parse the html."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986c050e80183127"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "['kristoffer lind glavind', 'matthew deverna', 'sho cho', 'balazs vedres', 'manju bura', 'anya hommadova lu', 'sagar kumar', 'piotr bródka', 'laura maria alessandretti', 'salvatore giorgi', 'naoki yoshinaga', 'ashlyn b. aske', 'nakao ran', 'sanja scepanovic', 'camille testard', 'sharon kang', 'alexandra segerberg', 'angelo brayner', 'michael cook', 'shaun bevan', 'laura boeschoten', 'yuan zhang', 'indrajeet patil', 'michael szell', 'daniele rama', 'anna rogers', 'neeley pate', 'rob chew', 'jon roozenbeek', 'louis boucherie', 'kai-cheng yang', 'xindi wang', 'brenda curtis', 'ryan louis stevens', 'nalette brodnax', 'andrea failla', 'miriam hurtado bodell', 'daniel larremore', 'lisette espin-noboa', 'zishan lan', 'edmond awad', 'vadim voskresenskii', 'yu-wen chen', 'katayoun farrahi', 'zoe k. rahwan', 'dan dai', 'ivano bison', 'd. sunshine hillygus', 'filipi nascimento silva', 'miriam redi', 'zhemeng xie', 'gemma read', 'gerardo iñiguez', 'cynthia rudin', 'yuan liao', 'fredrik jansson', 'antonio rojas-garcia', 'gianni barlacchi', 'levin brinkmann', 'marios constantinides', 'nicolò pagan', 'masaru kitsuregawa', 'mariana macedo', 'agnieszka falenska', 'caterina suitner', 'sahar baribi-bartov', 'andreas spitz', 'francesco lamperti', 'lester mackey', 'mariano gastón beiró', 'annapaola marconi', 'ana maria jaramillo', 'isabella loaiza', 'sarah ita levitan', 'stephanie eckman', 'leon fröhling', 'aguru ishibashi', 'christine vega pourheydarian', 'micah epstein', 'vedran sekara', 'hannah metzler', 'simone centellegher', 'amy munz', 'aaron clauset', 'darren croft', 'janet b. pierrehumbert', 'leonardo rosado', 'frederik riedel', 'regina widjaya', 'sonja m schmer-galunder', 'takako hashimoto', 'artem kuriksha', 'andreia sofia teixeira', 'quinn lanners', 'rémi suchonn', 'niel hens', 'andrea baronchelli', ' bokányi', 'tony carden', 'aymeric luneau']\n"
     ]
    }
   ],
   "source": [
    "# Get the HTML content\n",
    "link = 'https://ic2s2-2023.org/program'\n",
    "r = requests.get(link)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all the lists of things going on\n",
    "programs = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "# Find all lists of authors\n",
    "program_authors = [program.find_all(\"i\") for program in programs]\n",
    "\n",
    "# Get the text\n",
    "all_authors = [author.text for authors in program_authors for author in authors]\n",
    "\n",
    "# Get the individual names\n",
    "all_names = [name.lower() for authors in all_authors for name in authors.split(\", \")]\n",
    "all_names_unique = list(set(all_names))\n",
    "\n",
    "print(len(all_names_unique))\n",
    "print(all_names_unique[:100])\n",
    "\n",
    "# Save file as json\n",
    "with open('authors.json', 'w') as f:\n",
    "    json.dump(all_names_unique, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:01:39.341622700Z",
     "start_time": "2024-02-21T14:01:38.523893700Z"
    }
   },
   "id": "24f81999a1824ada",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. How many unique researches do you get?\n",
    "\n",
    "As seen in the code output above we get 1472 unique authors.\n",
    "\n",
    "6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retrieve as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices?\n",
    "\n",
    "We first go to the webpage to inspect the url. We notice that all the programs are in elements marked 'ul' with class name 'nav_list'. We get these programs and find all the names in them, contained in the elements marked 'i'. Then it's just a matter of formatting the resulting lists and deleting duplicates. The approach of taking all the navigable lists ensures that we get names from all the types of events. We look at a subset of the names, to see that it is indeed names and not something else we have found."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5862e97df87e7b36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2\n",
    "\n",
    "1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book.\n",
    "\n",
    "For the custom-made data used by Centola, there are the usual downsides to this type of data: it probably took quite a lot of effort to set up and design the experiment, and they couldn't be sure beforehand how many people they would get to sign up to the network. On the other hand, they got the exact, specific data they wanted, and were thus able to get more out of their limited data.\n",
    "\n",
    "For the ready-made data used in the second study, there was the advantage that all the data was collected beforehand. Thus, the researchers could jump right to analyzing it. The volume of data from the fitness tracker would most likely also be way bigger than anything they could have designed themselves. The fact that data is collected over time also provides the added advantage of facilitating longitudinal analysis. The downside is, that since they had no control over the collection process, they had to be more careful when doing their causal inference, as some variables could not be measured directly and the biases of the collection process is harder to uncover.\n",
    "\n",
    "2. How do you think these differences can influence the interpretation of the results in each study?\n",
    "\n",
    "With ready-made data, one will often have to do some data-manipulation to infer the variables of interest. This can introduce added uncertainty to the conclusions drawn from the study. On the other hand, with custom-made data, one will often have to extrapolate/do inference from fewer data-points, since the collection process is more ardous. This can limit how value of the study's conclusions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab0e58ed3d73069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8b5c3f024e203c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
