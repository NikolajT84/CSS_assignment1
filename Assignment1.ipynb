{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "\n",
    "Link to github: https://github.com/NikolajT84/CSS_assignment1\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "We have written the code working together on Nikolaj's machine, which is why all commits are from the same user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "175b6d70c0d147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports for all exercises\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:43:22.152477600Z",
     "start_time": "2024-02-21T14:43:22.004343400Z"
    }
   },
   "id": "75a3d5f27b9b93b1",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1\n",
    "\n",
    "We follow the instructions, and use bs4 to parse the html."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986c050e80183127"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "['kristoffer lind glavind', 'matthew deverna', 'sho cho', 'balazs vedres', 'manju bura', 'anya hommadova lu', 'sagar kumar', 'piotr bródka', 'laura maria alessandretti', 'salvatore giorgi', 'naoki yoshinaga', 'ashlyn b. aske', 'nakao ran', 'sanja scepanovic', 'camille testard', 'sharon kang', 'alexandra segerberg', 'angelo brayner', 'michael cook', 'shaun bevan', 'laura boeschoten', 'yuan zhang', 'indrajeet patil', 'michael szell', 'daniele rama', 'anna rogers', 'neeley pate', 'rob chew', 'jon roozenbeek', 'louis boucherie', 'kai-cheng yang', 'xindi wang', 'brenda curtis', 'ryan louis stevens', 'nalette brodnax', 'andrea failla', 'miriam hurtado bodell', 'daniel larremore', 'lisette espin-noboa', 'zishan lan', 'edmond awad', 'vadim voskresenskii', 'yu-wen chen', 'katayoun farrahi', 'zoe k. rahwan', 'dan dai', 'ivano bison', 'd. sunshine hillygus', 'filipi nascimento silva', 'miriam redi', 'zhemeng xie', 'gemma read', 'gerardo iñiguez', 'cynthia rudin', 'yuan liao', 'fredrik jansson', 'antonio rojas-garcia', 'gianni barlacchi', 'levin brinkmann', 'marios constantinides', 'nicolò pagan', 'masaru kitsuregawa', 'mariana macedo', 'agnieszka falenska', 'caterina suitner', 'sahar baribi-bartov', 'andreas spitz', 'francesco lamperti', 'lester mackey', 'mariano gastón beiró', 'annapaola marconi', 'ana maria jaramillo', 'isabella loaiza', 'sarah ita levitan', 'stephanie eckman', 'leon fröhling', 'aguru ishibashi', 'christine vega pourheydarian', 'micah epstein', 'vedran sekara', 'hannah metzler', 'simone centellegher', 'amy munz', 'aaron clauset', 'darren croft', 'janet b. pierrehumbert', 'leonardo rosado', 'frederik riedel', 'regina widjaya', 'sonja m schmer-galunder', 'takako hashimoto', 'artem kuriksha', 'andreia sofia teixeira', 'quinn lanners', 'rémi suchonn', 'niel hens', 'andrea baronchelli', ' bokányi', 'tony carden', 'aymeric luneau']\n"
     ]
    }
   ],
   "source": [
    "# Get the HTML content\n",
    "link = 'https://ic2s2-2023.org/program'\n",
    "r = requests.get(link)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all the lists of things going on\n",
    "programs = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "# Find all lists of authors\n",
    "program_authors = [program.find_all(\"i\") for program in programs]\n",
    "\n",
    "# Get the text\n",
    "all_authors = [author.text for authors in program_authors for author in authors]\n",
    "\n",
    "# Get the individual names\n",
    "all_names = [name.lower() for authors in all_authors for name in authors.split(\", \")]\n",
    "all_names_unique = list(set(all_names))\n",
    "\n",
    "print(len(all_names_unique))\n",
    "print(all_names_unique[:100])\n",
    "\n",
    "# Save file as json\n",
    "with open('authors.json', 'w') as f:\n",
    "    json.dump(all_names_unique, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:01:39.341622700Z",
     "start_time": "2024-02-21T14:01:38.523893700Z"
    }
   },
   "id": "24f81999a1824ada",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. How many unique researches do you get?\n",
    "\n",
    "As seen in the code output above we get 1472 unique authors.\n",
    "\n",
    "6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retrieve as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices?\n",
    "\n",
    "We first go to the webpage to inspect the url. We notice that all the programs are in elements marked 'ul' with class name 'nav_list'. We get these programs and find all the names in them, contained in the elements marked 'i'. Then it's just a matter of formatting the resulting lists and deleting duplicates. The approach of taking all the navigable lists ensures that we get names from all the types of events. We look at a subset of the names, to see that it is indeed names and not something else we have found."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5862e97df87e7b36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2\n",
    "\n",
    "1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book.\n",
    "\n",
    "For the custom-made data used by Centola, there are the usual downsides to this type of data: it probably took quite a lot of effort to set up and design the experiment, and they couldn't be sure beforehand how many people they would get to sign up to the network. On the other hand, they got the exact, specific data they wanted, and were thus able to get more out of their limited data.\n",
    "\n",
    "For the ready-made data used in the second study, there was the advantage that all the data was collected beforehand. Thus, the researchers could jump right to analyzing it. The volume of data from the fitness tracker would most likely also be way bigger than anything they could have designed themselves. The fact that data is collected over time also provides the added advantage of facilitating longitudinal analysis. The downside is, that since they had no control over the collection process, they had to be more careful when doing their causal inference, as some variables could not be measured directly and the biases of the collection process is harder to uncover.\n",
    "\n",
    "2. How do you think these differences can influence the interpretation of the results in each study?\n",
    "\n",
    "With ready-made data, one will often have to do some data-manipulation to infer the variables of interest. This can introduce added uncertainty to the conclusions drawn from the study. On the other hand, with custom-made data, one will often have to extrapolate/do inference from fewer data-points, since the collection process is more ardous. This can limit how value of the study's conclusions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab0e58ed3d73069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "\n",
    "Here we use the author database that we collected in week 2.\n",
    "We first get all authors with works count between 5 and 5000."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8b5c3f024e203c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "authors = pd.read_pickle('authors.pkl')\n",
    "authors = authors.loc[(authors['works_count'] > 5) &\n",
    "                      (authors['works_count'] < 5000)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:31:03.412357700Z",
     "start_time": "2024-02-21T14:31:03.406035500Z"
    }
   },
   "id": "5426a54ac88b5962",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then construct the filter for the concepts, ie.\n",
    "\"Sociology|Psychology|Economics|Political science\n",
    "& Mathematics|Physics|Computer science\"\n",
    "but using the concept ids, which we need to get."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2078d81bacf1960"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the base url\n",
    "base_url = 'https://api.openalex.org/works'\n",
    "\n",
    "# Columns for the dataframes\n",
    "cols_papers = ['id', 'publication_year', 'cited_by_count', 'author_ids']\n",
    "cols_abstract = ['id', 'title', 'abstract_inverted_index']\n",
    "\n",
    "# Produce the concept filter\n",
    "# First get all high-level concepts\n",
    "concepts_url = 'https://api.openalex.org/concepts'\n",
    "params_concepts = {'filter': 'level:0'}\n",
    "result_concepts = requests.get(concepts_url, params=params_concepts).json()['results']\n",
    "concepts = {concept['display_name']: concept['id'] for concept in result_concepts}\n",
    "\n",
    "# Then define the lists of each category\n",
    "soc_concepts = ['Sociology', 'Psychology', 'Economics', 'Political science']\n",
    "quant_concepts = ['Mathematics', 'Physics', 'Computer science']\n",
    "\n",
    "# Construct the conditions for \n",
    "condition_soc = '|'.join(concepts[c] for c in soc_concepts)\n",
    "condition_quant = '|'.join(concepts[c] for c in quant_concepts)\n",
    "\n",
    "# Construct filter\n",
    "concepts_filter_soc = f'concept.id:{condition_soc}'\n",
    "concepts_filter_quant = f'concept.id:{condition_quant}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:43:10.935435800Z",
     "start_time": "2024-02-21T14:43:09.305680300Z"
    }
   },
   "id": "353122bdd7892f4a",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define our filters and the dataframes, and define a function to get the works of a batch of authors.\n",
    "Doing it in this manner enables us to use parallelization to speed up the process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66106e27a1fc9dd1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect filters\n",
    "filters = ',cited_by_count:>10,authors_count:<10,' + concepts_filter_soc + ',' + concepts_filter_quant\n",
    "\n",
    "author_ids_list = list(authors['id_fixed'])\n",
    "\n",
    "# Produce batch indexes for querying authors in bulk\n",
    "batch_size = 10\n",
    "author_batches_idx = []\n",
    "for batch in range(0, len(authors), batch_size):\n",
    "    author_batches_idx.append((batch, min(batch + batch_size, len(authors))))\n",
    "\n",
    "# Define dataframes\n",
    "papers_all = pd.DataFrame(columns=cols_papers)\n",
    "abstract_all = pd.DataFrame(columns=cols_abstract)\n",
    "\n",
    "# Function to query the api\n",
    "def get_works(author_batch, author_ids_list):\n",
    "    i, j = author_batch\n",
    "    author_ids_str = '|'.join(author_ids_list[i:j])\n",
    "    params = {'filter': 'author.id:' + author_ids_str + filters,\n",
    "              'per-page': '200'}\n",
    "    next_cursor = '*'\n",
    "\n",
    "    # Define dataframes\n",
    "    papers_df = pd.DataFrame(columns=cols_papers)\n",
    "    abstract_df = pd.DataFrame(columns=cols_abstract)\n",
    "\n",
    "    # Flip through the pages\n",
    "    while next_cursor is not None:\n",
    "        params['cursor'] = next_cursor\n",
    "        result = requests.get(base_url, params=params).json()\n",
    "        works = result['results']\n",
    "        next_cursor = result['meta']['next_cursor']\n",
    "        for work in works:\n",
    "            # We take the characters [21:] from the ids in order to avoid the\n",
    "            # start of the url, and get only the id itself.\n",
    "            author_ids_work = [author['author']['id'][21:] for author in work['authorships']]\n",
    "            new_paper = pd.DataFrame([[work[key] if not key == 'author_ids' else author_ids_work\n",
    "                                       for key in cols_papers]],\n",
    "                                     columns=cols_papers)\n",
    "            new_abstract = pd.DataFrame([[work[key] for key in cols_abstract]],\n",
    "                                        columns=cols_abstract)\n",
    "            # Concatenate to dataframe\n",
    "            papers_df = pd.concat([papers_df, new_paper], ignore_index=True)\n",
    "            abstract_df = pd.concat([abstract_df, new_abstract], ignore_index=True)\n",
    "\n",
    "    return papers_df, abstract_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:43:13.074174200Z",
     "start_time": "2024-02-21T14:43:13.059239100Z"
    }
   },
   "id": "c008bb684971e29f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now run the query, using the recommend tricks of searching for multiple authors, plus parallel processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6501f1689a74f9b8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:46<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run queries in parallel\n",
    "result = Parallel(n_jobs=-1)(delayed(get_works)(author_batch, author_ids_list) \n",
    "                             for author_batch in tqdm(author_batches_idx))\n",
    "for pdf, adf in result:\n",
    "    papers_all = pd.concat([papers_all, pdf], ignore_index=True)\n",
    "    abstract_all = pd.concat([abstract_all, adf], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:44:20.644195300Z",
     "start_time": "2024-02-21T14:43:25.386947300Z"
    }
   },
   "id": "3dfb41a57d8ba7d6",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can save our data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c1eae885343254"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "papers_all.to_pickle('IC2S2_papers.pkl')\n",
    "abstract_all.to_pickle('IC2S2_abstracts.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:51:01.405508200Z",
     "start_time": "2024-02-21T14:51:00.550592800Z"
    }
   },
   "id": "55be66b58fc12d1d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Dataset summary. \n",
    "How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?\n",
    "\n",
    "We take a look:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8238ff88a64ec5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 10457\n",
      "Number of unique authors: 13810\n"
     ]
    }
   ],
   "source": [
    "print('Total number of papers: ' + str(len(papers_all)))\n",
    "\n",
    "paper_authors = set([ids for author_ids in papers_all['author_ids'] for ids in author_ids])\n",
    "print('Number of unique authors: ' + str(len(paper_authors)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:57:58.596579800Z",
     "start_time": "2024-02-21T14:57:58.577930400Z"
    }
   },
   "id": "7d882f5e72fcd44a",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the number of papers in the dataframe is 10457, and the 13810 unique authors authored these works."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad030084f4ab023"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Efficiency in code. \n",
    "Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?\n",
    "\n",
    "First of all, as much of the filtering as possible was frontloaded in the filters or in the selection of authors, in order to reduce the bottleneck, which was the API requests. Second of all, we searched by groups of authors. This reduced the overhead of connecting to the API for each author, and allowed us to pull more data each time. Lastly, we used parallel processing to use all the compute available. All these things sped up the process to the point where the job only took ~45 seconds, instead of upwards of an hour."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b0e1afc7fc980a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filtering Criteria and Dataset Relevance \n",
    "Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?\n",
    "\n",
    "Only taking authors with work counts between 5 and 5000 eliminates outliers, and only takes authors about whom it may be assumed that they have worked at least semi-regularly in CSS. The citation count ensures that the papers we collect have relevance in the field, and the amount of authors may again help to weed out outlier papers, that are not representative of the field. Using the filter for the different concepts ensures that the papers are indeed relevant to the field of CSS.\n",
    "\n",
    "These filters provide us with a dataset where we can be fairly sure, that all papers are relevant to CSS. However, newer researchers or those who work in smaller, less recognized fields might be underrepresented, which could become an issue."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "431531dac9fdaad2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
