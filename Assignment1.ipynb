{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1\n",
    "\n",
    "Link to github: https://github.com/NikolajT84/CSS_assignment1\n",
    "\n",
    "### Collaboration\n",
    "\n",
    "We have written the code working together on Nikolaj's machine, which is why all commits are from the same user."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "175b6d70c0d147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports for all exercises\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mean, mode, median"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:09:53.156349800Z",
     "start_time": "2024-02-21T16:09:53.135160500Z"
    }
   },
   "id": "75a3d5f27b9b93b1",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1\n",
    "\n",
    "We follow the instructions, and use bs4 to parse the html."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986c050e80183127"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1472\n",
      "[' bokÃ¡nyi', 'aaron clauset', 'aaron j. schwartz', 'aaron schein', 'aaron smith', 'abbas haidar', 'abby smith', 'abdulkadir celikkanat', 'abdullah almaatouq', 'abdullah zameek', 'abeer elbahrawy', 'adam finnemann', 'adam frank', 'adam h. russell', 'adam stefkovics', 'adam sutton', 'aditi dutta', 'adriano belisario', 'adrienne mendrik', 'agnieszka czaplicka']\n"
     ]
    }
   ],
   "source": [
    "# Get the HTML content\n",
    "link = 'https://ic2s2-2023.org/program'\n",
    "r = requests.get(link)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "# Find all the lists of things going on\n",
    "programs = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "# Find all lists of authors\n",
    "program_authors = [program.find_all(\"i\") for program in programs]\n",
    "\n",
    "# Get the text\n",
    "all_authors = [author.text for authors in program_authors for author in authors]\n",
    "\n",
    "# Get the individual names\n",
    "all_names = [name.lower() for authors in all_authors for name in authors.split(\", \")]\n",
    "all_names_unique = list(set(all_names))\n",
    "\n",
    "print(len(all_names_unique))\n",
    "all_names_unique.sort()\n",
    "print(all_names_unique[:20])\n",
    "\n",
    "# Save file as json\n",
    "with open('authors.json', 'w') as f:\n",
    "    json.dump(all_names_unique, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:10:21.573559800Z",
     "start_time": "2024-02-22T12:10:20.252203500Z"
    }
   },
   "id": "24f81999a1824ada",
   "execution_count": 124
  },
  {
   "cell_type": "markdown",
   "source": [
    "_5. How many unique researches do you get?_\n",
    "\n",
    "As seen in the code output above we get 1472 unique authors.\n",
    "\n",
    "_6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retrieve as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices?_\n",
    "\n",
    "We first go to the webpage to inspect the url. We notice that all the programs are in elements marked 'ul' with class name 'nav_list'. We get these programs and find all the names in them, contained in the elements marked 'i'. Then it's just a matter of formatting the resulting lists and deleting duplicates. The approach of taking all the navigable lists ensures that we get names from all the types of events. We look at a subset of the names, to see that it is indeed names and not something else we have found. We deem that since we will be using the search function of the openalex API to find authors, further checking the names is unnecessary, since name mispellings, duplicates etc. will be caught at that point."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5862e97df87e7b36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 2\n",
    "\n",
    "_1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book._\n",
    "\n",
    "For the custom-made data used by Centola, there are the usual downsides to this type of data: it probably took quite a lot of effort to set up and design the experiment, and they couldn't be sure beforehand how many people they would get to sign up to the network. On the other hand, they got the exact, specific data they wanted, and were thus able to get more out of their limited data.\n",
    "\n",
    "For the ready-made data used in the second study, there was the advantage that all the data was collected beforehand. Thus, the researchers could jump right to analyzing it. The volume of data from the fitness tracker would most likely also be way bigger than anything they could have designed themselves. The fact that data is collected over time also provides the added advantage of facilitating longitudinal analysis. The downside is, that since they had no control over the collection process, they had to be more careful when doing their causal inference, as some variables could not be measured directly and the biases of the collection process is harder to uncover.\n",
    "\n",
    "_2. How do you think these differences can influence the interpretation of the results in each study?_\n",
    "\n",
    "With ready-made data, one will often have to do some data-manipulation to infer the variables of interest. This can introduce added uncertainty to the conclusions drawn from the study. On the other hand, with custom-made data, one will often have to extrapolate/do inference from fewer data-points, since the collection process is more ardous. This can limit how value of the study's conclusions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab0e58ed3d73069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 3\n",
    "\n",
    "Here we use the author database that we collected in week 2.\n",
    "We first get all authors with works count between 5 and 5000."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8b5c3f024e203c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "authors = pd.read_pickle('authors.pkl')\n",
    "authors = authors.loc[(authors['works_count'] > 5) &\n",
    "                      (authors['works_count'] < 5000)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T15:50:52.166690700Z",
     "start_time": "2024-02-21T15:50:52.138547400Z"
    }
   },
   "id": "5426a54ac88b5962",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then construct the filter for the concepts, ie.\n",
    "\"Sociology|Psychology|Economics|Political science\n",
    "& Mathematics|Physics|Computer science\"\n",
    "but using the concept ids, which we need to get."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2078d81bacf1960"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the base url\n",
    "base_url = 'https://api.openalex.org/works'\n",
    "\n",
    "# Columns for the dataframes\n",
    "cols_papers = ['id', 'publication_year', 'cited_by_count', 'author_ids']\n",
    "cols_abstract = ['id', 'title', 'abstract_inverted_index']\n",
    "\n",
    "# Produce the concept filter\n",
    "# First get all high-level concepts\n",
    "concepts_url = 'https://api.openalex.org/concepts'\n",
    "params_concepts = {'filter': 'level:0'}\n",
    "result_concepts = requests.get(concepts_url, params=params_concepts).json()['results']\n",
    "concepts = {concept['display_name']: concept['id'] for concept in result_concepts}\n",
    "\n",
    "# Then define the lists of each category\n",
    "soc_concepts = ['Sociology', 'Psychology', 'Economics', 'Political science']\n",
    "quant_concepts = ['Mathematics', 'Physics', 'Computer science']\n",
    "\n",
    "# Construct the conditions for \n",
    "condition_soc = '|'.join(concepts[c] for c in soc_concepts)\n",
    "condition_quant = '|'.join(concepts[c] for c in quant_concepts)\n",
    "\n",
    "# Construct filter\n",
    "concepts_filter_soc = f'concept.id:{condition_soc}'\n",
    "concepts_filter_quant = f'concept.id:{condition_quant}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T15:26:32.787873100Z",
     "start_time": "2024-02-21T15:26:31.153081600Z"
    }
   },
   "id": "353122bdd7892f4a",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define our filters and the dataframes, and define a function to get the works of a batch of authors.\n",
    "Doing it in this manner enables us to use parallelization to speed up the process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66106e27a1fc9dd1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect filters\n",
    "filters = ',cited_by_count:>10,authors_count:<10,' + concepts_filter_soc + ',' + concepts_filter_quant\n",
    "\n",
    "author_ids_list = list(authors['id_fixed'])\n",
    "\n",
    "# Produce batch indexes for querying authors in bulk\n",
    "batch_size = 10\n",
    "author_batches_idx = []\n",
    "for batch in range(0, len(authors), batch_size):\n",
    "    author_batches_idx.append((batch, min(batch + batch_size, len(authors))))\n",
    "\n",
    "# Define dataframes\n",
    "papers_all = pd.DataFrame(columns=cols_papers)\n",
    "abstract_all = pd.DataFrame(columns=cols_abstract)\n",
    "\n",
    "# Function to query the api\n",
    "def get_works(author_batch, author_ids_list):\n",
    "    i, j = author_batch\n",
    "    author_ids_str = '|'.join(author_ids_list[i:j])\n",
    "    params = {'filter': 'author.id:' + author_ids_str + filters,\n",
    "              'per-page': '200'}\n",
    "    next_cursor = '*'\n",
    "\n",
    "    # Define dataframes\n",
    "    papers_df = pd.DataFrame(columns=cols_papers)\n",
    "    abstract_df = pd.DataFrame(columns=cols_abstract)\n",
    "\n",
    "    # Flip through the pages\n",
    "    while next_cursor is not None:\n",
    "        params['cursor'] = next_cursor\n",
    "        result = requests.get(base_url, params=params).json()\n",
    "        works = result['results']\n",
    "        next_cursor = result['meta']['next_cursor']\n",
    "        for work in works:\n",
    "            # We take the characters [21:] from the ids in order to avoid the\n",
    "            # start of the url, and get only the id itself.\n",
    "            author_ids_work = [author['author']['id'][21:] for author in work['authorships']]\n",
    "            new_paper = pd.DataFrame([[work[key] if not key == 'author_ids' else author_ids_work\n",
    "                                       for key in cols_papers]],\n",
    "                                     columns=cols_papers)\n",
    "            new_abstract = pd.DataFrame([[work[key] for key in cols_abstract]],\n",
    "                                        columns=cols_abstract)\n",
    "            # Concatenate to dataframe\n",
    "            papers_df = pd.concat([papers_df, new_paper], ignore_index=True)\n",
    "            abstract_df = pd.concat([abstract_df, new_abstract], ignore_index=True)\n",
    "\n",
    "    return papers_df, abstract_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:43:13.074174200Z",
     "start_time": "2024-02-21T14:43:13.059239100Z"
    }
   },
   "id": "c008bb684971e29f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now run the query, using the recommend tricks of searching for multiple authors, plus parallel processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6501f1689a74f9b8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 104/104 [00:46<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run queries in parallel\n",
    "result = Parallel(n_jobs=-1)(delayed(get_works)(author_batch, author_ids_list) \n",
    "                             for author_batch in tqdm(author_batches_idx))\n",
    "for pdf, adf in result:\n",
    "    papers_all = pd.concat([papers_all, pdf], ignore_index=True)\n",
    "    abstract_all = pd.concat([abstract_all, adf], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:44:20.644195300Z",
     "start_time": "2024-02-21T14:43:25.386947300Z"
    }
   },
   "id": "3dfb41a57d8ba7d6",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can save our data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c1eae885343254"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "papers_all.to_pickle('IC2S2_papers.pkl')\n",
    "abstract_all.to_pickle('IC2S2_abstracts.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:51:01.405508200Z",
     "start_time": "2024-02-21T14:51:00.550592800Z"
    }
   },
   "id": "55be66b58fc12d1d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Dataset summary. \n",
    "_How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?_\n",
    "\n",
    "We take a look:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8238ff88a64ec5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 10457\n",
      "Number of unique authors: 13810\n"
     ]
    }
   ],
   "source": [
    "print('Total number of papers: ' + str(len(papers_all)))\n",
    "\n",
    "paper_authors = set([ids for author_ids in papers_all['author_ids'] for ids in author_ids])\n",
    "print('Number of unique authors: ' + str(len(paper_authors)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T14:57:58.596579800Z",
     "start_time": "2024-02-21T14:57:58.577930400Z"
    }
   },
   "id": "7d882f5e72fcd44a",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the number of papers in the dataframe is 10457, and the 13810 unique authors authored these works."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad030084f4ab023"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Efficiency in code. \n",
    "_Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time?_\n",
    "\n",
    "First of all, as much of the filtering as possible was frontloaded in the filters or in the selection of authors, in order to reduce the bottleneck, which was the API requests. Second of all, we searched by groups of authors. This reduced the overhead of connecting to the API for each author, and allowed us to pull more data each time. Lastly, we used parallel processing to use all the compute available. All these things sped up the process to the point where the job only took ~45 seconds, instead of upwards of an hour."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64b0e1afc7fc980a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Filtering Criteria and Dataset Relevance \n",
    "_Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices?_\n",
    "\n",
    "Only taking authors with work counts between 5 and 5000 eliminates outliers, and only takes authors about whom it may be assumed that they have worked at least semi-regularly in CSS. The citation count ensures that the papers we collect have relevance in the field, and the amount of authors may again help to weed out outlier papers, that are not representative of the field. Using the filter for the different concepts ensures that the papers are indeed relevant to the field of CSS.\n",
    "\n",
    "These filters provide us with a dataset where we can be fairly sure, that all papers are relevant to CSS. However, newer researchers or those who work in smaller, less recognized fields might be underrepresented, which could become an issue."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "431531dac9fdaad2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 4\n",
    "We first construct the graph from the edge list."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6edd6bea607fc34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "author_set = set(authors['id_fixed'])\n",
    "edge_dict = {}\n",
    "\n",
    "for co_authors in papers_all['author_ids']:\n",
    "    # Check that we only use authors from the author file\n",
    "    co_authors = list(set(co_authors) & author_set)\n",
    "    # Leave out papers with only one author\n",
    "    if len(co_authors) < 2:\n",
    "        continue\n",
    "    co_authors.sort()\n",
    "    author_pairs = list(combinations(co_authors, r=2))\n",
    "    for author_pair in author_pairs:\n",
    "        if author_pair not in edge_dict:\n",
    "            edge_dict[author_pair] = 1\n",
    "        else:\n",
    "            edge_dict[author_pair] += 1\n",
    "\n",
    "edge_list = [[author_pair[0], author_pair[1], weight]\n",
    "             for author_pair, weight in list(zip(edge_dict.keys(),\n",
    "                                             edge_dict.values()))]\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edge_list, weight='weight')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:15:08.690288Z",
     "start_time": "2024-02-21T16:15:08.652720100Z"
    }
   },
   "id": "5aaeb7326d56167e",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need the first year of publication:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c7fc8c1ac7cb853"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get first year of publication\n",
    "first_pub = papers_all.explode('author_ids').groupby(['author_ids'])['publication_year'].min()\n",
    "first_pub_df = pd.DataFrame(first_pub)\n",
    "first_pub_df.index.name = 'idx'\n",
    "first_pub_df.columns = ['first_pub']\n",
    "first_pub_df['id_fixed'] = first_pub_df.index\n",
    "authors = pd.merge(authors, first_pub_df, on='id_fixed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T15:50:59.255273600Z",
     "start_time": "2024-02-21T15:50:57.744286100Z"
    }
   },
   "id": "2fa51f6fc44baf15",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we can add the note attributes and save the network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56a5354c177abb51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "attribute_dict = {author['id_fixed']: {'display_name': author['display_name'],\n",
    "                                       'country': author['country_code'],\n",
    "                                       'citations_count': author['citations_count'],\n",
    "                                       'first_pub': author['first_pub']}\n",
    "                  for i, author in authors.iterrows()}\n",
    "nx.set_node_attributes(G, attribute_dict)\n",
    "\n",
    "# Write to list\n",
    "data = nx.readwrite.json_graph.node_link_data(G)\n",
    "\n",
    "# Write the data to a json file\n",
    "with open('graph.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:15:11.517942200Z",
     "start_time": "2024-02-21T16:15:11.444527800Z"
    }
   },
   "id": "9d8cf814ccaab2a",
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Network Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "834a54dd510ad5fb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  405\n",
      "Number of edges:  754\n",
      "Density:  0.009216477203275883\n",
      "Fully connected:  False\n",
      "Number of components:  42\n",
      "Number of isolates:  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes: ', len(G.nodes))\n",
    "print('Number of edges: ', len(G.edges))\n",
    "print('Density: ', nx.density(G))\n",
    "print('Fully connected: ', nx.is_connected(G))\n",
    "print('Number of components: ', len(list(nx.connected_components(G))))\n",
    "print('Number of isolates: ', len(list(nx.isolates(G))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T07:57:10.341328600Z",
     "start_time": "2024-02-22T07:57:10.332241800Z"
    }
   },
   "id": "50e39b2bd2d2c9e2",
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the calculations above we see that the network has 405 nodes and 754 links. The density is 0.009 which is low - meaning the network is indeed sparse - which is expected since papers take a long time to write, and most authors have only written a handful, and thereby have only collaborated with a tiny fraction of the whole field. We also see that the network is not fully connected, and that there are 42 connected components. Since we constructed the graph from an edge list only there are no isolates.\n",
    "\n",
    "All in all this seems to be what one would expect. For the reasons given above it makes sense that the network is sparse, and for geographical/institutional reasons it makes sense that there are more than a few connected components, despite the globalized nature of modern research."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e09ab6467db577"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Degree Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87e059e3f05a5870"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree mean:  3.723456790123457\n",
      "Degree median:  2\n",
      "Degree mode:  1\n"
     ]
    }
   ],
   "source": [
    "degrees = [d for _, d in list(G.degree)]\n",
    "print('Degree mean: ', mean(degrees))\n",
    "print('Degree median: ',median(degrees))\n",
    "print('Degree mode: ',mode(degrees))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T07:57:12.874608900Z",
     "start_time": "2024-02-22T07:57:12.859665600Z"
    }
   },
   "id": "1d3c573c938e90f9",
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strength mean:  25.145679012345678\n",
      "Strength median:  12\n",
      "Strength mode:  2\n"
     ]
    }
   ],
   "source": [
    "strength = [d for _, d in list(G.degree(weight='weight'))]\n",
    "print('Strength mean: ',mean(strength))\n",
    "print('Strength median: ',median(strength))\n",
    "print('Strength mode: ',mode(strength))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T07:57:14.732608Z",
     "start_time": "2024-02-22T07:57:14.713745400Z"
    }
   },
   "id": "ced174182fc54498",
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the degree analysis we see that there are many nodes with low degree/strength, and fewer nodes with higher values (mean>median>mode). This makes sense, in that there are few authors with many papers and collaborations (often in leadership positions) and many authors with fewer papers, probably lots of PhD students and the like."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95cc45531a25a8df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Top Authors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d6be9a4919f2c36"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five authors:\n",
      "Ciro Cattuto :  22\n",
      "Nicola Perra :  20\n",
      "Sune Lehmann :  19\n",
      "Filippo Menczer :  18\n",
      "Alain Barrat :  18\n"
     ]
    }
   ],
   "source": [
    "sort_idxs = sorted(range(len(degrees)),key=degrees.__getitem__)\n",
    "degrees_sorted = [list(G.degree)[i] for i in sort_idxs]\n",
    "top_five = degrees_sorted[:-6:-1]\n",
    "print('Top five authors:')\n",
    "for id, degree in top_five:\n",
    "    author_name = authors.loc[authors['id_fixed']==id]['display_name'].values[0]\n",
    "    print(author_name, ': ', degree)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T08:20:35.418127600Z",
     "start_time": "2024-02-22T08:20:35.370425100Z"
    }
   },
   "id": "66f0516a4164107a",
   "execution_count": 118
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do a little research on each author:\n",
    "\n",
    "__Ciro Cattuto__: Scientific Director of ISI Foundation, and a founder and principal investigator of the SocioPatterns collaboration. _\"My work focuses on measuring and understanding complex phenomena in systems that entangle human behaviors and digital platforms.\"_\n",
    "\n",
    "__Nicola Perra__: _\"I serve as Reader in Applied Mathematics at Queen Mary University of London, UK and chair of the British Chapter of the network Society.\"_\n",
    "\n",
    "__Sune Lehmann__: _\"Iâm a Professor of Networks and Complexity Science at DTU Compute, Technical University of Denmark. Iâm also a Professor of Social Data Science at the Center for Social Data Science (SODAS), University of Copenhagen.\"_\n",
    "\n",
    "__Filippo Menczer__: University Distinguished Professor and the Luddy Professor of Informatics and Computer Science at the Luddy School of Informatics, Computing, and Engineering, Indiana University.\n",
    "\n",
    "__Alain Barrat__: Senior researcher affiliated with CNRS, CPT, and Turing Center for Living systems in Marseille, France.\n",
    "\n",
    "__Comments__: We would say all of these seem highly related to CSS, based on their descriptions of their work. These are the authors with the most works collaborated on within the authors dataset, and it makes sense that most of them occupy leadership positions in institutes focused on CSS or related, since getting authorships on hundreds of papers probably mean that you oversee a lot of projects. Given that these are the most heavily connected authors, they serve as 'hubs' within their institutes and the field at large. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5dfdfe1ac0d11ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
